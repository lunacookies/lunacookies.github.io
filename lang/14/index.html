<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Part Fourteen: Comments · arzg’s website</title><link rel=stylesheet href=https://arzg.github.io/scss/main.5b8c013dd5e51a40b743f06124716408dad09d87641389dd50c9b9d63c648588.css integrity="sha256-W4wBPdXlGkC3Q/BhJHFkCNrQnYdkE4ndUMm51jxkhYg="><script src=https://unpkg.com/quicklink@2.0.0/dist/quicklink.umd.js></script>
<script src=https://unpkg.com/anchor-js@4.3.1/anchor.min.js></script>
<script src=https://unpkg.com/prismjs@1.25.0/components/prism-core.min.js></script>
<script src=https://unpkg.com/prismjs@1.25.0/plugins/autoloader/prism-autoloader.min.js></script>
<script>window.onload=()=>{quicklink.listen()},document.addEventListener("DOMContentLoaded",function(a){anchors.add("main h1")})</script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest></head><body><nav class=site-navigation><ul><li><a href=/>Home</a></li><li><a href=/blog/>Blog</a></li><li class=current><a href=/lang/>Make A Language</a></li></ul></nav><header class=header-area><h1 class=title>Part Fourteen: Comments</h1><section class=page-info><ul><li>10 December 2020</li><li>672 words</li><li>three minute read</li></ul></section></header><main><p>The first thing we need to do is teach the lexer to recognise comments. We’ll begin with a test:</p><pre><code class=language-rust>// lexer.rs

#[cfg(test)]
mod tests {
    // snip

    #[test]
    fn lex_comment() {
        check(&quot;# foo&quot;, SyntaxKind::Comment);
    }
}
</code></pre><p>Here’s the implementation:</p><pre><code class=language-rust>pub(crate) enum SyntaxKind {
    // snip

    #[regex(&quot;#.*&quot;)]
    Comment,

    #[error]
    Error,

    Root,
    BinaryExpr,
    PrefixExpr,
}
</code></pre><p>Take note of how we aren’t using <code>#[logos::skip]</code> here; instead, we are explicitly including comments in the output of our lexer. We do this to ensure that the parser fully contains the input text, which makes the parser <em>lossless.</em> This makes implementing tools that interact with the source text (a good example is automatic refactorings in an IDE) easier to implement.</p><p>Just like with whitespace, it would be nice if we don’t have to manually handle comments in the parser. We could add extra checks to our existing <code>eat_whitespace</code> methods on <code>Sink</code> and <code>Source</code> for comments, but that’s annoying. What if we have other token kinds that we want to automatically skip in future?</p><p>There’s a name for this kind of irrelevant token: <em>trivia.</em> As far as I can tell, the term comes from <a href=https://github.com/dotnet/roslyn>Roslyn</a>. Let’s add an <code>is_trivia</code> method to <code>SyntaxKind</code> to abstract away this behaviour:</p><pre><code class=language-rust>impl SyntaxKind {
    pub(crate) fn is_trivia(self) -&gt; bool {
        matches!(self, Self::Whitespace | Self::Comment)
    }
}
</code></pre><p>Note how the method takes <code>self</code>; this is because it’s more efficient to pass <code>SyntaxKind</code> by value instead of by reference, since the size of <code>SyntaxKind</code> is one byte, which is less than the size of a reference (eight bytes on 64-bit systems). Also note that <code>is_trivia</code> won’t consume the instance of <code>SyntaxKind</code>, since <code>SyntaxKind</code> is <code>Copy</code>.</p><p>Now that we have a way to ask a <code>SyntaxKind</code> if it is trivia, we can use this method in <code>Sink</code> and <code>Source</code>:</p><pre><code class=language-rust>// source.rs

impl&lt;'l, 'input&gt; Source&lt;'l, 'input&gt; {
    // snip

    pub(super) fn next_lexeme(&amp;mut self) -&gt; Option&lt;&amp;'l Lexeme&lt;'input&gt;&gt; {
        self.eat_trivia();

        let lexeme = self.lexemes.get(self.cursor)?;
        self.cursor += 1;

        Some(lexeme)
    }

    pub(super) fn peek_kind(&amp;mut self) -&gt; Option&lt;SyntaxKind&gt; {
        self.eat_trivia();
        self.peek_kind_raw()
    }

    fn eat_trivia(&amp;mut self) {
        while self.at_trivia() {
            self.cursor += 1;
        }
    }

    fn at_trivia(&amp;self) -&gt; bool {
        self.peek_kind_raw().map_or(false, SyntaxKind::is_trivia)
    }

    // snip
}
</code></pre><pre><code class=language-rust>// sink.rs

impl&lt;'l, 'input&gt; Sink&lt;'l, 'input&gt; {
    // snip

    pub(super) fn finish(mut self) -&gt; GreenNode {
        // snip

        for event in reordered_events {
            match event {
                // snip
            }

            self.eat_trivia();
        }

        // snip
    }

    fn eat_trivia(&amp;mut self) {
        while let Some(lexeme) = self.lexemes.get(self.cursor) {
            if !lexeme.kind.is_trivia() {
                break;
            }

            self.token(lexeme.kind, lexeme.text.into());
        }
    }

    // snip
}
</code></pre><p>Let’s write a test to find out if what we’ve made works:</p><pre><code class=language-rust>// parser.rs

#[cfg(test)]
mod tests {
    // snip

    #[test]
    fn parse_comment() {
        check(
            &quot;# hello!&quot;,
            expect![[r##&quot;
Root@0..8
  Comment@0..8 &quot;# hello!&quot;&quot;##]],
        );
    }
}
</code></pre><p>The usage of an extra <code>#</code> in the raw string literal is to stop Rust from thinking that the <code>"#</code> in <code>Comment@0..8 "#</code> is meant to end the string literal.</p><pre><code class=language-->$ cargo t -q
running 34 tests
..................................
test result: ok. 34 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre><p>Let’s try parsing a binary expression interspersed with comments:</p><pre><code class=language-rust>// expr.rs

#[cfg(test)]
mod tests {
    // snip

    #[test]
    fn parse_binary_expression_interspersed_with_comments() {
        check(
            &quot;
1
  + 1 # Add one
  + 10 # Add ten&quot;,
            expect![[r##&quot;
Root@0..35
  Whitespace@0..1 &quot;\n&quot;
  BinaryExpr@1..35
    BinaryExpr@1..21
      Number@1..2 &quot;1&quot;
      Whitespace@2..5 &quot;\n  &quot;
      Plus@5..6 &quot;+&quot;
      Whitespace@6..7 &quot; &quot;
      Number@7..8 &quot;1&quot;
      Whitespace@8..9 &quot; &quot;
      Comment@9..18 &quot;# Add one&quot;
      Whitespace@18..21 &quot;\n  &quot;
    Plus@21..22 &quot;+&quot;
    Whitespace@22..23 &quot; &quot;
    Number@23..25 &quot;10&quot;
    Whitespace@25..26 &quot; &quot;
    Comment@26..35 &quot;# Add ten&quot;&quot;##]],
        );
    }

    // snip
}
</code></pre><p>The test fails, since we aren’t lexing newlines. Let’s write a test for this:</p><pre><code class=language-rust>// lexer.rs

#[cfg(test)]
mod tests {
    use super::*;

    fn check(input: &amp;str, kind: SyntaxKind) {
        // snip
    }

    #[test]
    fn lex_spaces_and_newlines() {
        check(&quot;  \n &quot;, SyntaxKind::Whitespace);
    }

    // snip
}
</code></pre><pre><code class=language-rust>pub(crate) enum SyntaxKind {
    #[regex(&quot;[ \n]+&quot;)]
    Whitespace,

    // snip
}
</code></pre><p>All our tests pass now:</p><pre><code class=language-->$ cargo t -q
running 35 tests
...................................
test result: ok. 35 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre><p>In the next part we’ll introduce another new concept to our parser: markers.</p></main><nav class=page-navigation><div class=prev><p class=hint>Previously</p><a href=https://arzg.github.io/lang/13/>Part Thirteen: Whitespace & Events</a></div><div class=next><p class=hint>Next up</p><a href=https://arzg.github.io/lang/15/>Part Fifteen: Markers</a></div><div style=clear:both></div></nav></body></html>